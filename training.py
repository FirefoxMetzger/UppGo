from keras.losses import mean_squared_error, categorical_crossentropy
from keras.optimizers import adam

from network import model
from load_data import loadTrainingData, SupervisedGoBatches

import keras
logger = keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)


loss_function = {"policy":categorical_crossentropy, "value":mean_squared_error}

model.compile(
    adam(),
    loss=loss_function,
    loss_weights={"policy":1,"value":0.001}
)

training_data_files = "replays/training_set/*.sgf"
training_data = loadTrainingData(training_data_files)
feeder = SupervisedGoBatches(training_data, 512)

model.fit_generator(
    feeder,
    epochs=2,
    callbacks=[logger]
)

# final 2 million games of self-play data generated by previous run of AlphaZero
#aneahling rate, momentum, regularization hyperparameter as in supervised learning experiment
# equal weight on cross entropy and MSE

# mini batch size = 2048 ; 3.1 million mini batches; 40 days training
# 2048 elements split in units of 32 across 64 GPUs
# use the most recent 500,000 games and sample uniformly
# momentum = 0.9
# checkpoint every 1000 steps; may be used to generate new data

# learning rates
# 0 - 400k      0.01
# 400k - 600k   0.001
# >600k         0.0001