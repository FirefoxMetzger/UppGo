from keras.losses import mean_squared_error, categorical_crossentropy
from keras.optimizers import adam
from keras.metrics import categorical_accuracy, top_k_categorical_accuracy
from keras.callbacks import ModelCheckpoint
from pathlib import Path
import keras

from network import AlphaZero
from load_data import create_dataset, ReplayQueue

import tensorflow as tf

session = tf.Session()
keras.backend.set_session(session)

batch_size = 64

# --- Dataset Creation ---
dataset_generator = create_dataset(
    Path("numpy/training"),
    Path("numpy/test"),
    Path("numpy/validation"),
)
training_dataset = tf.data.Dataset.from_generator(
    dataset_generator["training"]["feeder"].__iter__,
    (tf.float32, tf.float32, tf.float32),
    (tf.TensorShape([19, 19, 17]), tf.TensorShape([362]), tf.TensorShape([]))
).shuffle(1000).batch(batch_size)

validation_dataset = tf.data.Dataset.from_generator(
    dataset_generator["validation"]["feeder"].__iter__,
    (tf.float32, tf.float32, tf.float32),
    (tf.TensorShape([19, 19, 17]), tf.TensorShape([362]), tf.TensorShape([]))
).batch(batch_size).prefetch(100)

test_dataset = tf.data.Dataset.from_generator(
    dataset_generator["test"]["feeder"].__iter__,
    (tf.float32, tf.float32, tf.float32),
    (tf.TensorShape([19, 19, 17]), tf.TensorShape([362]), tf.TensorShape([]))
).batch(batch_size)


# --- Iterator Creation ---
iterator = tf.data.Iterator.from_structure(training_dataset.output_types,
                                           training_dataset.output_shapes)
example, action, value = iterator.get_next()
train_init_op = iterator.make_initializer(training_dataset)
validation_init_op = iterator.make_initializer(validation_dataset)
test_init_op = iterator.make_initializer(test_dataset)


# --- Model Creation ---
def moves_predicted(y_true, y_pred):
    val = categorical_accuracy(y_true, y_pred)
    return val


def moves_top10(y_true, y_pred):
    val = top_k_categorical_accuracy(y_true, y_pred, k=10)
    return val

input_vals = keras.layers.Input(tensor=example)
model = AlphaZero(input_vals, residual_blocks=2)
model.compile(
    adam(),
    loss={
        "policy": categorical_crossentropy,
        "value": mean_squared_error
    },
    loss_weights={
        "policy": 1,
        "value": 0.01
    },
    metrics={
        "policy": [moves_predicted, moves_top10],
        "value": []
    },
    target_tensors=[
        action,
        value
    ]
)

# --- Training ---
combined_loss = list()
correct_moves = list()
top10_moves = list()

for epochs in range(3):
    session.run(train_init_op)
    model.fit(
        epochs=1,
        steps_per_epoch=1000
    )

    session.run(validation_init_op)
    result = model.evaluate(steps=100)
    combined_loss.append(result[0])
    correct_moves(result[3])
    top10_moves(result[4])

    model.save(str(Path("models/model-%d-epochs.hdf5" % epochs)))
    np.save(str("models/loss.npy"), np.array(combined_loss))
    np.save(str("models/correct_moves.npy"), np.array(correct_moves))
    np.save(str("models/top10_moves.npy"), np.array(top10_moves))

session.run(test_init_op)
result = model.evaluate(steps=100)

print(result)

# final 2 million games of self-play data generated by previous run of AlphaZero
#aneahling rate, momentum, regularization hyperparameter as in supervised learning experiment
# equal weight on cross entropy and MSE

# mini batch size = 2048 ; 3.1 million mini batches; 40 days training
# 2048 elements split in units of 32 across 64 GPUs
# use the most recent 500,000 games and sample uniformly
# momentum = 0.9
# checkpoint every 1000 steps; may be used to generate new data

# learning rates
# 0 - 400k      0.01
# 400k - 600k   0.001
# >600k         0.0001