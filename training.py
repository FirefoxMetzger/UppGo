from keras.losses import mean_squared_error, categorical_crossentropy
from keras.optimizers import adam
from keras.metrics import categorical_accuracy, top_k_categorical_accuracy
from keras.callbacks import ModelCheckpoint
import keras
from pathlib import Path

from network import model
from load_data import create_dataset, ReplayQueue

import tensorflow as tf


logger = keras.callbacks.TensorBoard(log_dir='./logs',
                                     histogram_freq=0,
                                     batch_size=32,
                                     write_graph=True,
                                     write_grads=False,
                                     write_images=False,
                                     embeddings_freq=0,
                                     embeddings_layer_names=None,
                                     embeddings_metadata=None)


loss_function = {"policy": categorical_crossentropy, 
                 "value": mean_squared_error}


def moves_predicted(y_true, y_pred):
    val = categorical_accuracy(y_true, y_pred)
    return val


def moves_top10(y_true, y_pred):
    val = top_k_categorical_accuracy(y_true, y_pred, k=10)
    return val

batch_size = 256

dataset_generator = create_dataset(
    Path("numpy/training"),
    Path("numpy/test"),
    Path("numpy/validation"),
)
dataset = tf.data.Dataset.from_generator(
    dataset_generator["training"]["feeder"].__iter__,
    (tf.bool, tf.float32, tf.float32),
    (tf.TensorShape([19, 19, 17]),tf.TensorShape([362]), tf.TensorShape([1]))
)
dataset = dataset.batch(5)

examples, actions, results = dataset.make_one_shot_iterator().get_next()

print(type(model.layers[0]))
model.layers[0] = keras.engine.topology.Input(tensor=examples)

model.compile(
    adam(),
    loss=loss_function,
    loss_weights={"policy": 1,
                  "value": 0.01},
    metrics={
        "policy": [moves_predicted, moves_top10],
        "value": []
    },
    target_tensors=[
        actions,
        results
    ]
)

print("--- Starting to fit the model ---")

model.fit(
    epochs=1,
    callbacks=[
        logger,
        ModelCheckpoint("./models/keras_model_ep_{epoch:02d}-{loss:.2f}.hdf5")
    ]
)

print("--- Starting to evaluate the model ---")

print(eval_result)

# final 2 million games of self-play data generated by previous run of AlphaZero
#aneahling rate, momentum, regularization hyperparameter as in supervised learning experiment
# equal weight on cross entropy and MSE

# mini batch size = 2048 ; 3.1 million mini batches; 40 days training
# 2048 elements split in units of 32 across 64 GPUs
# use the most recent 500,000 games and sample uniformly
# momentum = 0.9
# checkpoint every 1000 steps; may be used to generate new data

# learning rates
# 0 - 400k      0.01
# 400k - 600k   0.001
# >600k         0.0001