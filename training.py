from keras.losses import mean_squared_error, categorical_crossentropy
from keras.optimizers import adam
from keras.metrics import categorical_accuracy, top_k_categorical_accuracy
from keras.callbacks import ModelCheckpoint

from network import model
from load_data import loadData, SupervisedGoBatches

import keras
logger = keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)


loss_function = {"policy":categorical_crossentropy, "value":mean_squared_error}
def moves_predicted(y_true, y_pred):
    val = categorical_accuracy(y_true,y_pred)
    return val
def moves_top10(y_true,y_pred):
    val = top_k_categorical_accuracy(y_true,y_pred,k=10)
    return val

model.compile(
    adam(),
    loss=loss_function,
    loss_weights={"policy":1,"value":0.001},
    metrics={
        "policy":[moves_predicted, moves_top10],
        "value":[]
    }
)

print("--- Loading Training Data ---")
training_data_files = "replays/training_set/*.sgf"
training_data = loadData(training_data_files)
feeder = SupervisedGoBatches(training_data, 512)

print("--- Loading Test Data ---")

test_data_files = "replays/training_set/*.sgf"
test = loadData(test_data_files)
test_feeder = SupervisedGoBatches(test, 512)

print("--- Starting to fit the model ---")

model.fit_generator(
    feeder,
    steps_per_epoch=3,
    epochs=2,
    callbacks=[
        logger,
        ModelCheckpoint("./models/keras_model_ep_{epoch:02d}-{loss:.2f}.hdf5")
    ]
)

print("--- Starting to evaluate the model ---")

model.evaluate_generator(
    test_feeder
)

# final 2 million games of self-play data generated by previous run of AlphaZero
#aneahling rate, momentum, regularization hyperparameter as in supervised learning experiment
# equal weight on cross entropy and MSE

# mini batch size = 2048 ; 3.1 million mini batches; 40 days training
# 2048 elements split in units of 32 across 64 GPUs
# use the most recent 500,000 games and sample uniformly
# momentum = 0.9
# checkpoint every 1000 steps; may be used to generate new data

# learning rates
# 0 - 400k      0.01
# 400k - 600k   0.001
# >600k         0.0001